{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27121d69",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\moza3\\anaconda3\\lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "curses is not supported on this machine (please install/reinstall curses for an optimal experience)\n"
     ]
    }
   ],
   "source": [
    "# things we need for NLP\n",
    "import nltk\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "stemmer = LancasterStemmer()\n",
    "\n",
    "# things we need for Tensorflow\n",
    "import numpy as np\n",
    "import tflearn\n",
    "import tensorflow as tf\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb87043f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import our chat-bot intents file\n",
    "import json\n",
    "with open('chatbot intents.json')as f:\n",
    "    intents = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6404a548",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "146 documents\n",
      "42 classes ['Bike parking and repair stands', 'City parking lots', 'Holiday parking regulations', 'Motorcycle, scooter, and e-scooter parking', 'Motorhome and tour bus parking', 'On street parking restrictions', 'Parking infractions and associated fines', 'Parking map', 'Parking permits', 'Parking strategy and studies', 'Parking ticket options', 'Parking tickets payment', 'Pay and display machines', 'Pay for parking by phone', 'Paying on foot', 'Report a problem for pay and display machines', 'Report an Illegally parked vehicle', 'Requesting a trial date', 'Review a parking ticket', 'Road Maintenance', 'Road and sidewalk maintenance reports', 'Seasonal beach parking passes', 'Snow plowing and clearing', 'Ticket Images', 'Time of provincial offences court offices', 'Towed cars', 'Winter weather parking ban', 'about', 'afternoon', 'deputization program', 'evening', 'fact-18', 'goodbye', 'greeting', 'infraction number', 'morning', 'neutral-response', 'night', 'no-response', 'provincial offences court offices', 'spring clean-up and street sweeping', 'thanks']\n",
      "175 unique stemmed words [\"'s\", '(', ')', 'a', 'abandon', 'about', 'afternoon', 'an', 'and', 'anim', 'any', 'anyon', 'ar', 'at', 'au', 'avail', 'ban', 'basin', 'beach', 'bicyc', 'bik', 'bonjo', 'bus', 'by', 'bye', 'cal', 'can', 'car', 'cart', 'catch', 'child', 'city', 'clean-up', 'condit', 'consult', 'court', 'dat', 'dead', 'deficy', 'deput', 'dis', 'display', 'do', 'doe', 'dur', 'e-scooter', 'ev', 'far', 'fin', 'find', 'foot', 'for', 'get', 'good', 'goodby', 'group', 'gut', 'hav', 'heal', 'hello', 'help', 'hey', 'hi', 'hol', 'holiday', 'how', 'howdy', 'i', 'if', 'illeg', 'im', 'in', 'infract', 'is', 'issu', 'it', 'konnichiw', 'lat', 'light', 'lot', 'machin', 'maint', 'map', 'me', 'ment', 'mor', 'morn', 'motorcyc', 'motorhom', 'much', 'municip', 'my', 'myself', 'nam', 'night', 'noth', 'numb', 'off', 'ok', 'ol', 'on', 'on-street', 'op', 'opt', 'or', 'park', 'pass', 'pathway', 'pay', 'permit', 'phon', 'plow', 'pothol', 'problem', 'profess', 'program', 'provint', 'pscg', 'rack', 'reg', 'repair', 'report', 'request', 'restrict', 'review', 'revoir', 'right', 'road', 'sayonar', 'scoot', 'season', 'see', 'shop', 'should', 'sidewalk', 'snow', 'spac', 'spring', 'stakehold', 'stand', 'start', 'strategy', 'street', 'study', 'sweep', 'tag', 'tel', 'than', 'thank', 'that', 'the', 'then', 'ther', 'ticket', 'tim', 'to', 'tour', 'tow', 'tri', 'us', 'vehic', 'very', 'want', 'was', 'weath', 'wel', 'what', 'when', 'wher', 'who', 'wint', 'with', 'yo', 'you', 'yourself']\n"
     ]
    }
   ],
   "source": [
    "words = []\n",
    "classes = []\n",
    "documents = []\n",
    "ignore_words = ['?' , ',' , '!' , '.' , ';' , ':']\n",
    "# loop through each sentence in our intents patterns\n",
    "for intent in intents['intents']:\n",
    "    for pattern in intent['patterns']:\n",
    "        # tokenize each word in the sentence\n",
    "        w = nltk.word_tokenize(pattern)\n",
    "        # add to our words list\n",
    "        words.extend(w)\n",
    "        # add to documents in our corpus\n",
    "        documents.append((w, intent['tag']))\n",
    "        # add to our classes list\n",
    "        if intent['tag'] not in classes:\n",
    "            classes.append(intent['tag'])\n",
    "\n",
    "# stem and lower each word and remove duplicates\n",
    "words = [stemmer.stem(w.lower()) for w in words if w not in ignore_words]\n",
    "words = sorted(list(set(words)))\n",
    "\n",
    "# remove duplicates\n",
    "classes = sorted(list(set(classes)))\n",
    "\n",
    "print (len(documents), \"documents\")\n",
    "print (len(classes), \"classes\", classes)\n",
    "print (len(words), \"unique stemmed words\", words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94e79ef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\moza3\\AppData\\Local\\Temp\\ipykernel_33232\\1193693976.py:27: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  training = np.array(training)\n"
     ]
    }
   ],
   "source": [
    "# create our training data\n",
    "training = []\n",
    "output = []\n",
    "# create an empty array for our output\n",
    "output_empty = [0] * len(classes)\n",
    "\n",
    "# training set, bag of words for each sentence\n",
    "for doc in documents:\n",
    "    # initialize our bag of words\n",
    "    bag = []\n",
    "    # list of tokenized words for the pattern\n",
    "    pattern_words = doc[0]\n",
    "    # stem each word\n",
    "    pattern_words = [stemmer.stem(word.lower()) for word in pattern_words]\n",
    "    # create our bag of words array\n",
    "    for w in words:\n",
    "        bag.append(1) if w in pattern_words else bag.append(0)\n",
    "\n",
    "    # output is a '0' for each tag and '1' for current tag\n",
    "    output_row = list(output_empty)\n",
    "    output_row[classes.index(doc[1])] = 1\n",
    "\n",
    "    training.append([bag, output_row])\n",
    "\n",
    "# shuffle our features and turn into np.array\n",
    "random.shuffle(training)\n",
    "training = np.array(training)\n",
    "\n",
    "# create train and test lists\n",
    "train_x = list(training[:,0])\n",
    "train_y = list(training[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "342bc933",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 18999  | total loss: \u001b[1m\u001b[32m0.00068\u001b[0m\u001b[0m | time: 0.048s\n",
      "| Adam | epoch: 1000 | loss: 0.00068 - acc: 1.0000 -- iter: 144/146\n",
      "Training Step: 19000  | total loss: \u001b[1m\u001b[32m0.00063\u001b[0m\u001b[0m | time: 0.050s\n",
      "| Adam | epoch: 1000 | loss: 0.00063 - acc: 1.0000 -- iter: 146/146\n",
      "--\n",
      "INFO:tensorflow:C:\\Users\\moza3\\model.tflearn is not in all_model_checkpoint_paths. Manually adding it.\n"
     ]
    }
   ],
   "source": [
    "# reset underlying graph data\n",
    "tf.compat.v1.reset_default_graph()\n",
    "# Build neural network\n",
    "net = tflearn.input_data(shape=[None, len(train_x[0])])\n",
    "net = tflearn.fully_connected(net, 8)\n",
    "net = tflearn.fully_connected(net, 8)\n",
    "net = tflearn.fully_connected(net, len(train_y[0]), activation='softmax')\n",
    "net = tflearn.regression(net)\n",
    "\n",
    "# Define model and setup tensorboard\n",
    "model = tflearn.DNN(net, tensorboard_dir='tflearn_logs')\n",
    "# Start training (apply gradient descent algorithm)\n",
    "model.fit(train_x, train_y, n_epoch=1000, batch_size=8, show_metric=True)\n",
    "model.save('model.tflearn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29f87ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f1e10e4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6009 (pid 13300), started 18:26:03 ago. (Use '!kill 13300' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-79e69185456b25fe\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-79e69185456b25fe\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6009;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir tflearn_logs --port=6009"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac1488e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_up_sentence(sentence):\n",
    "    # tokenize the pattern\n",
    "    sentence_words = nltk.word_tokenize(sentence)\n",
    "    # stem each word\n",
    "    sentence_words = [stemmer.stem(word.lower()) for word in sentence_words]\n",
    "    return sentence_words\n",
    "\n",
    "# return bag of words array: 0 or 1 for each word in the bag that exists in the sentence\n",
    "def bow(sentence, words, show_details=False):\n",
    "    # tokenize the pattern\n",
    "    sentence_words = clean_up_sentence(sentence)\n",
    "    # bag of words\n",
    "    bag = [0]*len(words)  \n",
    "    for s in sentence_words:\n",
    "        for i,w in enumerate(words):\n",
    "            if w == s: \n",
    "                bag[i] = 1\n",
    "                if show_details:\n",
    "                    print (\"found in bag: %s\" % w)\n",
    "\n",
    "    return(np.array(bag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f05e9721",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "p = bow('What causes mental illness?', words)\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7011fa23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save all of our data structures\n",
    "import pickle\n",
    "pickle.dump( {'words':words, 'classes':classes, 'train_x':train_x, 'train_y':train_y}, open( \"training_data\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70255c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
